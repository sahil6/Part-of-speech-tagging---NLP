#!/usr/bin/python3
###################################
# CS B551 Fall 2018, Assignment #3
#

'''
The following three approaches were used for part of speech tagging:
1.	Naïve Bayes method
2.	Viterbi algorithm
3.	MCMC with Gibbs sampling

As a first step we train the data, and store the emission, transition probabilities in a dictionary.
The dictionaries store:
1.	Probability of word given speech P(W|S),
2.	transition probabilities P(Sn|Sn-1) and P(Sn|Sn-1, Sn-2)

The above probabilities are stored to use further for the algos.

NAÏVE BAYES: Generates most likely tag sequence using naive bayes inference.
Here each part of speech is considered independent of the other
Formulation: P(S|W) = max. P(W|S)*P(S)/P(W)
As P(W) is constant, we ignore it and assign the tag with which we get maximum probability

VITERBI ALGORITHM:

Viterbi is based on Dynamic programming for part of speech tagging.
Formulations of the problem: Part of speech tagging is very common application of Viterbi algorithm. In part of speech
tagging we can only observe the words and we need to determine the Part of speech of that word. Here the words are
observed variables and hidden variables are the parts of speeches.
For Viterbi we learned the transition probabilities and emission probabilities from the training file. Then applied
that trained parameters to the testing inputs. This problem can also be solved by using trigram transition probabilities,
but we used the just bigram to keep the things simple.

Viterbi is defined as:
V(k , v)  = max of all parts of speeches calculation { V(k-1, u) * q(v | u) * e(x, v) }
Where k is = sequence of length k
Given sequence ending with  part of speech v (for length k)
The part of speech u, we compute for all parts of speeches we have
q= the transition probability from u to v
e(x , v) = the emission probability for given word(x) and part of speech(v)

Posterior is calculated as =
P(S1)*{P(S2|S1)P(S3|S2)…P(Sn|Sn-1)}{  P(W1|S1)…P(Wn|Sn)}
Where s denotes the part of speeches and W denotes words

How program works:  We have used the list of dictionaries to represent the Viterbi table.
The key of the dictionary part of speech we are currently calculating for. The index of the Viterbi list means the
word for which we plan to find the part of speech. For given part of speech and word we calculate based on all the
parts of speeches and take the maximum value of all and then this value is assigned to cell of the Viterbi table
(for given part of speech and word). The path this algorithm take is store in another dictionary and at each step new
part of speech is added to the Viterbi path. At the end we compute the maximum and the path for this maximum part of
speech is returned, which is the path our Viterbi algorithm takes.
Design Decision: Everything is changed to lower case to make the comparisons case insensitive. The transition counts
are store in one dictionary using which the transition probabilities are calculated and emission counts are stored in
different dictionary from which the emission probabilities are calculated.
Results of this evaluation on bc.test file were:
1.     With grammar function:
a.     Sentence accuracy: 60.65%
b.     Word accuracy: 96.09%
2.     Without grammar function:
a.     Sentence accuracy: 54.45%
b.     Word accuracy: 95.05%


MCMC GIBBS SAMPLING:
Here the model assumes a more complicated Bayes net with every part of speech depending on the previous two parts
of speech

Algo/Formulation:
1.	Assign a random set of parts of speech to each word, and then choose each word and assign it all the 12 parts of
speech. For each word we compute the posterior given by the following formula:

2.	P(Si|S1…Si-1,Si+1…Sn, W1…Wn) or P(Si|S-Si, W)  =
marginalised over Si (P (S1)P(W1|S1)P(S2|S1)P(S3|S1,S2)….P(Sn|Sn-1,Sn-2)

3.	Rearranging the above terms:
P(S1)*{P(S2|S1)P(S3|S2)…P(Sn|Sn-1)}*{P(S3|S1,S2)….P(Sn|Sn-1,Sn-2)}{P(W1|S1)…P(Wn|Sn)}

4.	The above terms are marginalised and the probability with each tag ie P(Si = Noun) and so on
5.	Next we randomly assign the tag each of which has a weight in proportion to its probability.
6.	Assign the picked parts of speech for the word and use this modified value for rest of the calculations
7.	We do this for all the words once, and then create a sample. This is done over a few hundred to thousand iterations.
8.	The first few samples are discarded and the from the rest the tag that occur the most for a word is assigned as
    the final tag.
Results of this evaluation on bc.test file were:
1.	With grammar function:
a.	Sentence accuracy: 94.51%
b.	Word accuracy: 52.21%
2.	Without grammar function:
a.	Sentence accuracy: 93.4%
b.	Word accuracy: 45.5%
Design decisions:
Initial Sample: To use the sample that was generated by the Naïve bayes model. (The burn in rate and iterations are
therefore kept low here as we saw by observation that the model converged very soon by doing this)
Number of iterations: The number of iterations for an arbitrarily taken initial sample are obviously more than
for a more informed initial sample(like the one I got from the naïve bayes model). It is observed that it converges
fairly fast in the latter case. For eg: a 50 iterations
Log probabilities:
    a.	We encountered problems of the probabilities getting extremely small and a result becoming zero during MCMC.
    This was solved by taking their log probabilities


Overall Design decisions:
1.	New word handling:
    a.	The new words are given certain probabilities according to the rules defined in the
    grammar_rules(these probabilities were arrived at empirically by running the code a few times)
    b.	Certain suffixes have a higher probability of being a certain word, so the new words were given
    probability according to this
    c.	Prefixes were avoided because there was a higher chance of mis tagging (from observation)
    d.	The list of suffixes I got was from here: https://web2.uvcs.uvic.ca/elc/sample/beginner/gs/gs_55_1.htm
    e.	If it didn’t fit any of the new conditions it was checked if it was a number, otherwise it was assigned a noun
    with a certain probability which was again derived empirically
2.	Data structure
    a.	Dictionary – we used dictionaries mostly because of the easy access to their values and constant time for fetching
    b.	Hashing of the already computed probabilities

Results:
As a result it was observed that there was a substantial increase in accuracy as a result of using the grammar
function on bc.train set

Without the grammar_rules function(the code has to be modified for this a bit):
So far scored 2000 sentences with 29442 words.
                   Words correct:     Sentences correct:
   0. Ground truth:      100.00%              100.00%
         1. Simple:       91.51%               36.35%
            2. HMM:       95.05%               54.45%
        3. Complex:       93.41%               45.85%
With the grammar_rules function:
                   Words correct:     Sentences correct:
   0. Ground truth:      100.00%              100.00%
         1. Simple:       93.79%               45.95%
            2. HMM:       96.09%               60.65%
        3. Complex:       94.51%               52.20%
One interesting observation is that even though the MCMC model is more thorough in that it takes dependencies from the previous to previous part of speech, it still has lower probability than HMM. My suspection is that it happens because of overfitting on the training set. Also I’ve learnt that Viterbi usually gives a highest accuracy of 97% using its trigram model

Calculation of the posterior and observations:
The posterior was calculated according to the formulae described above for each model on the solution provided by the other model.
It can be easily observed that Viterbi and Naïve Bayes return the highest value of their probabilities for their own solutions,
however this may not always be the case with MCMC(when it doesn’t converge sometimes) because it is a probabilistic model.
Here is an example:
	         Simple     HMM Complex something flailed at   the  side of   nick's head as   they rolled around and  around .
0. Ground truth   -51.69  -48.60  -58.02 noun      verb    adp  det  noun adp  noun   noun adp  pron verb   adv    conj adv    .
      1. Simple   -50.78  -50.01  -59.35 noun      det     adp  det  noun adp  det    noun adp  pron verb   adv    conj adv    .
         2. HMM   -50.98  -47.61  -56.09 noun      noun    adp  det  noun adp  det    noun adp  pron verb   adv    conj adv    .
     3. Complex   -51.38  -47.67  -56.30 noun      .       adp  det  noun adp  det    noun adp  pron verb   adv    conj adv    .

Here we notice that MCMC assigns the highest probability to the HMM, this can be however fixed though by continuing
the iterations to a larger number.



'''
####



import random
import math




# transition counts  = matrix with count from Noun to Verb
# initial counts: how many sentenses are starting with Noun

global_emission_count = {}
global_transition_count = {}
global_2nd_level_transition = {}
global_initial_count_for_part_of_speech = {}
global_unique_parts_of_speeches = []
viterbi_table = [{}]

global_probabilities_emission = {}
global_probabilities_transition = {}
global_probabilities_2nd_level_transition = {}

# We've set up a suggested code structure, but feel free to change it. Just
# make sure your code still works with the label.py and pos_scorer.py code
# that we've supplied.

class Solver:
    
    def add_emission_count(self,word, part_of_speech):
        # accessing the global emission count in this function
        global global_emission_count
        # if that word already exist in the dictionary
        if word in global_emission_count:
            # if there is word and also there exist the corresponding part of speech
            if part_of_speech in global_emission_count[word]:
                global_emission_count[word][part_of_speech] = global_emission_count[word][part_of_speech] + 1
            else:
                # there exist the word but there is no corresponding part of speech
                global_emission_count[word][part_of_speech] = 1
        # if word doesn't exist in the dictionary, adding the new word
        else:
            global_emission_count[word] = { part_of_speech : 1}
    
    def add_transition_count(self, part_of_speech1, part_of_speech2):
        
        global global_transition_count
        if part_of_speech1 in global_transition_count:
            if part_of_speech2 in global_transition_count[part_of_speech1]:
                global_transition_count[part_of_speech1][part_of_speech2] = global_transition_count[part_of_speech1][part_of_speech2] + 1
            else:
                global_transition_count[part_of_speech1][part_of_speech2] = 1
        else:
            global_transition_count[part_of_speech1] = {part_of_speech2 : 1}

    # this computes the number of transtions from State Sn ->Sn+1 -> Sn+2
    def second_level_transition(self, part_of_speech1, part_of_speech2, part_of_speech3):
        global global_2nd_level_transition
        if part_of_speech1 in global_2nd_level_transition:
            if part_of_speech2 in global_2nd_level_transition[part_of_speech1]:
                if part_of_speech3 in global_2nd_level_transition[part_of_speech1][part_of_speech2]:
                    global_2nd_level_transition[part_of_speech1][part_of_speech2][part_of_speech3] = global_2nd_level_transition[part_of_speech1][part_of_speech2][part_of_speech3] + 1
                else:
                    global_2nd_level_transition[part_of_speech1][part_of_speech2][part_of_speech3]= 1
            else:
                global_2nd_level_transition[part_of_speech1][part_of_speech2] = {part_of_speech3 : 1}
        else:
            global_2nd_level_transition[part_of_speech1]= {part_of_speech2:{part_of_speech3 : 1}}

    
    # when the part of speech is at the beginning of the sentence
    def add_initial_count_for_part_of_speech(self, part_of_speech):
        
        if part_of_speech in global_initial_count_for_part_of_speech:
            global_initial_count_for_part_of_speech[part_of_speech] = global_initial_count_for_part_of_speech[part_of_speech] + 1
        else:
            global_initial_count_for_part_of_speech[part_of_speech] = 1
    
    # for the transition probability ie P(Sn|Sn-1)
    def get_transition_probability(self, part_of_speech1, part_of_speech2):
        if part_of_speech1 in global_probabilities_transition and part_of_speech2 in global_probabilities_transition[part_of_speech1]:
            return global_probabilities_transition[part_of_speech1][part_of_speech2]
        
        if part_of_speech1 in global_transition_count and part_of_speech2 in global_transition_count[part_of_speech1] and part_of_speech2 in global_transition_count:
            value = global_transition_count[part_of_speech1][part_of_speech2] / sum(global_transition_count[part_of_speech1].values())
            return value
        return 0.0000001

    # P(w|s)
    def get_emission_probability(self,word, part_of_speech):
        if word in global_probabilities_emission and part_of_speech in global_probabilities_emission[word]:
            return global_probabilities_emission[word][part_of_speech]
        
        if word in global_emission_count and part_of_speech in global_emission_count[word] and part_of_speech in global_transition_count:
            value = global_emission_count[word][part_of_speech] / sum(global_transition_count[part_of_speech].values())
            global_probabilities_emission[word] = {part_of_speech : value }
            return value

        return self.grammar_rules(word,part_of_speech)

# https://web2.uvcs.uvic.ca/elc/sample/beginner/gs/gs_55_1.htm
# Our model tries to take advantage of some simple grammar rules in the English language
    def grammar_rules(self, word, tag):
        p = 0.9
        if word not in global_emission_count:
            if (list(word)[-3:] == list("ing") or list(word)[-2:] == list("ed") or list(word)[-3:] == list("ify")) and tag == 'verb':
                return p

            if (list(word)[-4:] == list("like") or list(word)[-4:] == list("less") or list(word)[-4:] == list("able")
                or list(word)[-3:] == list("ful") or list(word)[-3:] == list("ous")or list(word)[-3:] == list("ish") or
                list(word)[-2:] == list("ic") or list(word)[-3:] == list("ive")) and tag == 'adj':
                return p

            if (list(word)[-2:] == list("ly") ) and tag == 'adv':
                return p

            if (list(word)[-2:] == list("'s") or list(word)[-3:] == list("ist") or list(word)[-3:] == list("ion") or
                list(word)[-4:] == list("ment"))and tag == 'noun':
                return p

            if tag == 'noun':
    #            print("word:", word, "tag:", tag)
                return 0.7
            try:
                if int(word):
                    if tag == 'num':
                        return 1
            except ValueError:
                pass
        return 0.0000001

    def get_initial_probability(self, part_of_speech):
        if part_of_speech in global_initial_count_for_part_of_speech:
            return global_initial_count_for_part_of_speech[part_of_speech] / sum(global_initial_count_for_part_of_speech.values())
        return 0.00000001

    def get_2nd_level_trans_prob(self, pos1,pos2,pos3):
        
        if pos1 in global_probabilities_2nd_level_transition and pos2 in global_probabilities_2nd_level_transition[pos1] and pos3 in  global_probabilities_2nd_level_transition[pos1][pos2]:
            return global_probabilities_2nd_level_transition[pos1][pos2][pos3]
        
        if pos1 in global_2nd_level_transition and pos2 in global_2nd_level_transition[pos1] and pos3 in global_2nd_level_transition[pos1][pos2]:
            value = global_2nd_level_transition[pos1][pos2][pos3]/sum(global_2nd_level_transition[pos1][pos2].values())
            global_probabilities_2nd_level_transition[pos1]  = { pos2 : {pos3 : value}  }
            return value
        return 0.00000001

    # Calculate the log of the posterior probability of a given sentence
    #  with a given part-of-speech labeling. Right now just returns -999 -- fix this!
    def posterior(self, model, sentence, label):
        words = list(sentence)
        part_of_speeches = list(label)
        if model == "Simple":
           return self.calcprob_simple(words,part_of_speeches)
        elif model == "Complex":
           return self.calcprob_complex(words, part_of_speeches)
        elif model == "HMM":
            return self.get_viterbi_posterior(words,part_of_speeches)
        else:
            print("Unknown algo!")

    # Do the training!
    #
    def train(self, train_file):
        global global_unique_parts_of_speeches
        with open(train_file, encoding='utf-8') as f:
            lines = [l.lower() for l in f.readlines()]
            previous_part_of_speech = None
            before_the_previous_part_of_speech = None
            for line in lines:
                tokens = line.split()
                parts_of_speech = tokens[1::2]
                words = tokens[::2]
                word_length = len(words)
                self.add_initial_count_for_part_of_speech(parts_of_speech[0])
                for i in range(word_length):

                    self.add_emission_count(words[i], parts_of_speech[i])

                    if not previous_part_of_speech is None:
                        self.add_transition_count(previous_part_of_speech, parts_of_speech[i])

                    if (before_the_previous_part_of_speech and previous_part_of_speech) is not None:
                        self.second_level_transition(before_the_previous_part_of_speech,previous_part_of_speech,parts_of_speech[i])
                    previous_part_of_speech = parts_of_speech[i]
                    
                    try:
                        before_the_previous_part_of_speech = parts_of_speech[i-1]
                    except IndexError:
                        before_the_previous_part_of_speech = None


        global_unique_parts_of_speeches = list(global_transition_count.keys())
        
    #ref: https://github.com/llrs/Viterbi/blob/master/Viterbi.py
    def viterbi(self, words):
        viterbi_table = [{}]
        viterbi_track = {}
       
        
        #Probabilities for the first level because we don't have the transition probabilities at this level
        for pos in global_unique_parts_of_speeches:
            viterbi_table[0][pos] = self.get_initial_probability(pos) * self.get_emission_probability(words[0], pos)
            viterbi_track[pos] = [pos]
     
       #probabilities from the second level onwards
        for horizontal_level in range(1, len(words)):
            viterbi_table.append({})
            current_path = {}
            
            #current pos for which we want to calculate the probability
            for current_pos in global_unique_parts_of_speeches:
                max_value = 0
                #calulating for all the parts of speech because the previous part of speech can be anything.
                #then we will take the maximum value form all the parts of speech and assign to the current cell
                for pre_pos in global_unique_parts_of_speeches:
                    value = viterbi_table[horizontal_level-1][pre_pos] * self.get_transition_probability(pre_pos,current_pos) * self.get_emission_probability(words[horizontal_level],current_pos)
                    if value > max_value:
                        max_value = value
                        state = pre_pos
                viterbi_table[horizontal_level][current_pos] = max_value
                current_path[current_pos] = viterbi_track[state] + [current_pos]


            viterbi_track = current_path

        max_value = -math.inf
        last_level = len(words) - 1
        #taking the maximum probability and the corresponding part of speech
        #ALso, determine the path that must be taking based on the best part of speech found at the last level
        for pos in global_unique_parts_of_speeches:
            if viterbi_table[last_level][pos] >= max_value:
                max_value  = viterbi_table[last_level][pos]
                best_state = pos
        state = best_state
        return viterbi_track[state]


    # uses the simple bayes net where the parts of speech are independent of each other
    def simplified(self, words):
        tags_list =['']*len(words)
        for j in range(len(words)):
            p = 0
            for i in range(len(global_unique_parts_of_speeches)):
                tag = global_unique_parts_of_speeches[i]
                p_new = self.get_emission_probability(words[j],tag)*self.get_initial_probability(tag)
                if p_new > p:
                    tags_list[j] = tag
                    p = p_new
        return tags_list

    # returns the most probable sample according to it
    def complex_mcmc(self, words):
        samples = []
        count_tags_array = []
#        sample = ["noun"] * len(words)  # initial sample, all tags are nouns
        sample = self.simplified(words) # here I use the answer from the simplified model as the initial
        iterations= 150 # total number of iterations, can be changed according to the initial sample and observations
        burning_iteration = 30 #burnin iterations

        for i in range(iterations):
            sample = self.generate_sample(words, sample)
            if i >= burning_iteration:
                samples.append(sample)

        for j in range(len(words)):
            count_tags = {}
            for sample in samples:
                try:
                    count_tags[sample[j]] += 1
                except KeyError:
                    count_tags[sample[j]] = 1
            count_tags_array.append(count_tags)

        final_tags = [max(count_tags_array[i], key = count_tags_array[i].get) for i in range(len(words))]
        return [ tag.lower() for tag in final_tags ] 

    # returns the posterior for the simple model
    def calcprob_simple(self, words, tags):
         p = 0
         for i in range(len(words)):
             p += math.log10(self.get_emission_probability(words[i],tags[i])) + \
                 math.log10(global_initial_count_for_part_of_speech[tags[i]]/sum(global_initial_count_for_part_of_speech.values()))
         return p

    # returns the posterior for the HMM
    def get_viterbi_posterior(self,words,parts_of_speeches):
        
        prob_s1 = math.log(global_initial_count_for_part_of_speech[parts_of_speeches[0]] / sum(global_initial_count_for_part_of_speech.values()),10)
        z = 0
        t = 0
        for i in range(len(parts_of_speeches)):
        # z = z + math.log(self.get_emission_probability(words[i],sample[i]), 10)
            z += math.log(self.get_emission_probability(words[i], parts_of_speeches[i]),10)
            if i != 0:
                t += math.log(self.get_transition_probability(parts_of_speeches[i - 1], parts_of_speeches[i]),10)
                
        return prob_s1 + z + t

    # returns the posterior for the complex model
    # here log is used as we encountered an error when the probabilities got too low
    def calcprob_complex(self, words, sample):
        # P(S1)* {P(S2/S1)...P(Sn/Sn-1)*{P(W1/S1)....P(Wn/Sn)}{P(S3/S1, S2)....P(Sn/Sn,Sn-1}

        s1 = sample[0]
        # prob_s1 = math.log(global_initial_count_for_part_of_speech[s1]/sum(global_initial_count_for_part_of_speech.values()), 10)
        prob_s1 = math.log(global_initial_count_for_part_of_speech[s1] / sum(global_initial_count_for_part_of_speech.values()),10)
        # product of transition probabilities (or sum of their logarithms)
        # ie pi(P(s_i/s_i-1))
        t = 0
        z = 0
        r = 0

        for i in range(len(sample)):
            # z = z + math.log(self.get_emission_probability(words[i],sample[i]), 10)
            z += math.log(self.get_emission_probability(words[i], sample[i]),10)
            if i != 0:
                t += math.log(self.get_transition_probability(sample[i - 1], sample[i]),10)
            if i != 0 and i != 1:
                r += math.log(self.get_2nd_level_trans_prob(sample[i - 2], sample[i - 1], sample[i]),10)

        # this is the log of the probability with base 10
        return prob_s1+t+z+r

    # this generates a sample for the complex mcmc model
    def generate_sample(self, words, sample):
        tags = list(global_initial_count_for_part_of_speech.keys())
        for index in range(len(words)):
            probability_array = [0] * len(tags)
            log_probability_array = [0] * len(tags)
            for j in range(len(tags)):
                sample[index] = tags[j]
                log_probability_array[j] = self.calcprob_complex(words, sample)

            a = min(log_probability_array)
            for i in range(len(log_probability_array)):
                log_probability_array[i] -= a
                probability_array[i] = math.pow(10, log_probability_array[i])

            s = sum(probability_array)
            probability_array = [x / s for x in probability_array]
            rand = random.random()
            p = 0
            for i in range(len(probability_array)):
                p += probability_array[i]
                if rand < p:
                    sample[index] = tags[i]
                    break
        return sample

    def hmm_viterbi(self, words):
        path = self.viterbi(words)
        return path


    # This solve() method is called by label.py, so you should keep the interface the
    #  same, but you can change the code itself. 
    # It should return a list of part-of-speech labelings of the sentence, one
    #  part of speech per word.
    #
    def solve(self, model, sentence):
        words = list(sentence)
        if model == "Simple":
            return self.simplified(words)
#            return ["NOUN"] * len(words)
        elif model == "Complex":   
            return self.complex_mcmc(words)
        elif model == "HMM":
            return self.hmm_viterbi(words)
        else:
            print("Unknown algo!")
